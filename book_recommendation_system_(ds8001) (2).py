# -*- coding: utf-8 -*-
"""book recommendation system (ds8001).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YUudaelX2FJBQHuff-GD9Wvnc09sW1KR
"""

import pandas as pd
import numpy as npe
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

book = '/content/drive/MyDrive/Books.csv'
user = '/content/drive/MyDrive/Users.csv'
rating = '/content/drive/MyDrive/Ratings.csv'

book=pd.read_csv(book)
user=pd.read_csv(user)
rating=pd.read_csv(rating)

book.head()

user.head()

rating.head()

"""## data cleaning

Book data
"""

# describe shows the numerical columns in the dataset and performs some statisitics on the dataset like ( total count, top, frequency)
book.describe()

book.shape

book.columns

book.info() ## info shows the datatype of the each columns in the dataset.

book.isnull().sum() ## suming up all the null value (empty space) in each columns of the dataset

"""user dataset"""

user.describe()

user.shape

user.info()

user.columns

user.isnull().sum()

"""rating dataset"""

rating.describe()

rating.shape

rating.info()

rating.columns

rating.isnull().sum()

print("Initial shapes:")
print(f"Books: {book.shape}")
print(f"Users: {user.shape}")
print(f"Ratings: {rating.shape}\n")

print("Cleaning Books dataset...")

# Fix column names (sometimes they have extra spaces or weird casing)
book.columns = ['ISBN', 'Book-Title', 'Book-Author', 'Year-of-Publication',
                'Publisher', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']

# 1. Book-Author: fill rare missing with 'Unknown'
book['Book-Author'].fillna('Unknown', inplace=True)

# 2. Publisher: fill missing with 'Unknown'
book['Publisher'].fillna('Unknown', inplace=True)

# 3. Year-of-Publication: fix invalid entries
book['Year-of-Publication'] = pd.to_numeric(book['Year-of-Publication'], errors='coerce')

import numpy as np
# Known bad entries in this dataset (DK Publishing, Gallimard, etc.)
book.loc[book['ISBN'].isin(['078946697X', '0789466953', '2070426769']),
         ['Book-Title', 'Book-Author', 'Year-of-Publication', 'Publisher']] = np.nan

book.drop(book[book['ISBN'].isin(['078946697X', '0789466953', '2070426769'])].index,
          inplace=True)

# Fill remaining invalid years with median or mode (usually 1995-2000 range)
book['Year-of-Publication'].fillna(book['Year-of-Publication'].median(), inplace=True)
book['Year-of-Publication'] = book['Year-of-Publication'].astype(int)

# Optional: drop image columns if not needed for modeling (saves memory)
book.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

print(f"Books after cleaning: {book.shape}")
print("Missing values in Books:\n", book.isnull().sum(), "\n")

print("Cleaning Users dataset...")

# Fix column names
user.columns = ['User-ID', 'Location', 'Age']

# Age cleaning: realistic age range 10–90, rest → NaN then impute with median
user['Age'] = np.where((user['Age'] < 10) | (user['Age'] > 90), np.nan, user['Age'])

# Fill missing Age with median
median_age = user['Age'].median()
user['Age'].fillna(median_age, inplace=True)
user['Age'] = user['Age'].astype(int)

# Extract country from Location (optional but useful)
user['Country'] = user['Location'].str.split(',').str[-1].str.strip()
user['Country'] = user['Country'].replace({'': 'unknown', 'n/a': 'unknown'})

print(f"Users after cleaning: {user.shape}")
print("Missing values in Users:\n", user.isnull().sum(), "\n")

print("Cleaning Ratings dataset...")

rating.columns = ['User-ID', 'ISBN', 'Book-Rating']

# Remove ratings from users or books not present in the main datasets
rating = rating[rating['ISBN'].isin(book['ISBN'])]
rating = rating[rating['User-ID'].isin(user['User-ID'])]

# Book-Crossing has implicit (0) and explicit (1-10) ratings
# Many people filter out implicit ratings for collaborative filtering
print(f"Total ratings: {len(rating)}")
print(f"Ratings distribution:\n{rating['Book-Rating'].value_counts().sort_index()}")

# Optional: Keep only explicit ratings (1-10) for better recommendations
# rating = rating[rating['Book-Rating'] != 0]

print(f"Ratings after cleaning: {len(rating)}\n")

# Merge all for exploratory analysis
df = rating.merge(book, on='ISBN', how='left')
df = df.merge(user, on='User-ID', how='left')

print(f"Final merged dataframe shape: {df.shape}")
print(df.head())

plt.style.use('seaborn-v0_8')
fig = plt.figure(figsize=(20, 15))

# 1. Distribution of Book Ratings
plt.subplot(3, 3, 1)
sns.countplot(x='Book-Rating', data=df[df['Book-Rating'] != 0], palette='viridis')
plt.title('Distribution of Explicit Book Ratings (1-10)')
plt.xlabel('Rating')

# 2. Number of ratings per book (Top 20 most rated)
plt.subplot(1, 3, 2)
top_books = df['Book-Title'].value_counts()[:20]
sns.barplot(y=top_books.index, x=top_books.values, palette='magma')
plt.title('Top 20 Most Rated Books')
plt.xlabel('Number of Ratings')

# 3. Number of ratings per user (Top 20 most active users)
plt.subplot(1, 3, 3)
top_users = df['User-ID'].value_counts()[:20]
sns.barplot(y=top_users.index.astype(str), x=top_users.values, palette='coolwarm')
plt.title('Top 20 Most Active Users')
plt.xlabel('Number of Ratings')

# 4. Distribution of User Ages
plt.subplot(2, 3, 4)
sns.histplot(user['Age'], bins=40, kde=True, color='skyblue')
plt.title('Age Distribution of Users')
plt.xlabel('Age')

# 5. Publication Year Distribution (after cleaning)
plt.subplot(3, 3, 5)
sns.histplot(book['Year-of-Publication'], bins=50, color='green')
plt.title('Distribution of Publication Years')
plt.xlim(1900, 2010)

# 6. Top 10 Countries by Number of Users
plt.subplot(2, 3, 6)
top_countries = user['Country'].value_counts()[:10]
sns.barplot(y=top_countries.index, x=top_countries.values, palette='plasma')
plt.title('Top 10 Countries by Number of Users')
plt.xlabel('Number of Users')

plt.tight_layout()
plt.show()

print("\n=== KEY INSIGHTS FROM EDA ===")
print(f"Total unique books: {book['ISBN'].nunique():,}")
print(f"Total unique users: {user['User-ID'].nunique():,}")
print(f"Total ratings (including implicit): {len(rating):,}")
print(f"Sparsity: {1 - len(rating) / (book['ISBN'].nunique() * user['User-ID'].nunique()):.6%}")
print(f"Average ratings per user: {len(rating)/user['User-ID'].nunique():.2f}")
print(f"Average ratings per book: {len(rating)/book['ISBN'].nunique():.2f}")

print("Final filtering for meaningful recommendations...")
ratings_explicit = rating[rating['Book-Rating'] >= 1].copy()
user_counts = ratings_explicit['User-ID'].value_counts()
book_counts = ratings_explicit['ISBN'].value_counts()
active_users = user_counts[user_counts >= 5].index
popular_books = book_counts[book_counts >= 5].index
ratings_final = ratings_explicit[ratings_explicit['User-ID'].isin(active_users) &
                                ratings_explicit['ISBN'].isin(popular_books)]

print(f"Final dataset → {len(ratings_final):,} ratings, {ratings_final['User-ID'].nunique()} users, {ratings_final['ISBN'].nunique()} books\n")

# Create mappings
all_users = sorted(ratings_final['User-ID'].unique())
all_books = sorted(ratings_final['ISBN'].unique())
user_to_idx = {u: i for i, u in enumerate(all_users)}
book_to_idx = {b: i for i, b in enumerate(all_books)}
id_to_book = {i: b for b, i in book_to_idx.items()}

n_users = len(all_users)
n_books = len(all_books)

# Build rating matrix (dense for simplicity — we'll use it directly)
R = np.zeros((n_users, n_books))
for row in ratings_final.itertuples():
    R[user_to_idx[row._1], book_to_idx[row.ISBN]] = row._3  # row._1 = User-ID, row._3 = rating

print(f"Rating matrix R: {R.shape} → {100*np.count_nonzero(R)/R.size:.3f}% density\n")

# =============================================================================
# 1. GREEDY: Popularity-Based (Weighted Score) → Your Greedy Algorithm
# =============================================================================
print("1. GREEDY ALGORITHM → Popularity-Based Recommender")

def greedy_popularity_recommend(n=10):
    book_stats = ratings_final.groupby('ISBN')['Book-Rating'].agg(['count', 'mean'])
    book_stats['score'] = book_stats['mean'] * np.log1p(book_stats['count'])
    top_isbns = book_stats.sort_values('score', ascending=False).head(n).index

    result = []
    for isbn in top_isbns:
        title = book[book['ISBN']==isbn]['Book-Title'].iloc[0]
        result.append((title, book_stats.loc[isbn, 'score']))
    return result

print("Top 5 Greedy Recommendations:")
for i, (title, score) in enumerate(greedy_popularity_recommend(5), 1):
    print(f"   {i}. {title[:55]:55} → {score:.2f}")
print()

# =============================================================================
# 2. DIVIDE & CONQUER + DYNAMIC PROGRAMMING → Longest Common Subsequence (LCS) for Sequence Matching
# =============================================================================
print("2. DP → LCS-based Sequence Recommender (for reading history)")

def lcs_length(a, b):
    # Classic DP from your class
    m, n = len(a), len(b)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if a[i-1] == b[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
            else:
                dp[i][j] = max(dp[i-1][j], dp[i][j-1])
    return dp[m][n]

# Example: Two users' reading sequences (by author or genre)
user1_books = ratings_final[ratings_final['User-ID'] == all_users[0]]['ISBN'].tolist()[:10]
user2_books = ratings_final[ratings_final['User-ID'] == all_users[10]]['ISBN'].tolist()[:10]
similarity = lcs_length(user1_books, user2_books)
print(f"LCS Similarity between User 0 and User 10: {similarity} common books in sequence")
print()

import heapq
from collections import defaultdict

# =============================================================================
# 3. GRAPH ALGORITHMS → Build User Similarity Graph + Dijkstra for "Most Influential Reader Path"
# =============================================================================
print("3. GRAPH + DIJKSTRA → Find shortest path of influence in reader network")

# Build user-user graph: edge weight = number of common books (higher = more similar)
def build_user_graph(R, threshold=2):
    graph = defaultdict(list)
    common_books = R @ R.T  # dot product = number of common rated books
    for i in range(n_users):
        for j in range(i+1, n_users):
            if common_books[i,j] >= threshold:
                weight = 1 / (1 + common_books[i,j])  # smaller weight = more similar
                graph[i].append((j, weight))
                graph[j].append((i, weight))
    return graph

user_graph = build_user_graph(R, threshold=2)

def dijkstra_path(graph, start, end):
    dist = {i: float('inf') for i in range(n_users)}
    dist[start] = 0
    prev = {}
    pq = [(0, start)]
    while pq:
        d, u = heapq.heappop(pq)
        if d > dist[u]: continue
        for v, w in graph[u]:
            if dist[u] + w < dist[v]:
                dist[v] = dist[u] + w
                prev[v] = u
                heapq.heappush(pq, (dist[v], v))
    # reconstruct path
    path = []
    curr = end
    while curr in prev:
        path.append(curr)
        curr = prev[curr]
    path.append(start)
    return path[::-1]

path = dijkstra_path(user_graph, 0, 50)
print(f"Dijkstra: Shortest influence path from User 0 → User 50: {path}")
print("   → These users form a 'taste chain'!")
print()

import time
import numpy as np

# =============================================================================
# 4. MATRIX FACTORIZATION USING GRADIENT DESCENT — OPTIMIZED & FAST
# Now runs in SECONDS instead of HOURS
# =============================================================================
print("4. GRADIENT DESCENT → Matrix Factorization (Vectorized & Blazing Fast!)")

def matrix_factorization_GD_fast(R, k=15, steps=1000, alpha=0.0005, beta=0.02, tol=1e-4):
    """
    Vectorized Stochastic-ish Gradient Descent for Matrix Factorization
    Runtime: ~5–15 seconds on full filtered dataset (instead of 30+ minutes)
    """
    n_users, n_books = R.shape
    P = np.random.normal(scale=1./k, size=(n_users, k))
    Q = np.random.normal(scale=1./k, size=(n_books, k))

    # Find indices of observed ratings (this is the key speedup!)
    users_idx, books_idx = np.nonzero(R)
    ratings = R[users_idx, books_idx]
    n_ratings = len(ratings)

    print(f"Training Fast MF | k={k}, steps={steps}, observed ratings={n_ratings:,}")
    prev_rmse = float('inf')

    for step in range(steps):
        # Shuffle indices every epoch for better convergence
        perm = np.random.permutation(n_ratings)

        # Mini-batch update (full batch here, but only over observed entries)
        for idx in perm:
            i = users_idx[idx]  # user index
            j = books_idx[idx]  # book index
            r = ratings[idx]    # true rating

            prediction = np.dot(P[i,:], Q[j,:])
            eij = r - prediction

            # Gradient descent update (vectorized per rating)
            P[i,:] += alpha * (eij * Q[j,:] - beta * P[i,:])
            Q[j,:] += alpha * (eij * P[i,:] - beta * Q[j,:])

        # Compute RMSE every 100 steps
        if step % 100 == 0 or step == steps - 1:
            pred = P @ Q.T
            error = np.sum((R[R > 0] - pred[R > 0]) ** 2)
            rmse = np.sqrt(error / n_ratings)
            improvement = prev_rmse - rmse
            print(f"   Step {step:4d} → RMSE = {rmse:.4f} (Δ {improvement:+.5f})")
            prev_rmse = rmse

            # Early stopping
            if improvement < tol:
                print(f"   Early stopping at step {step} (converged)")
                break

    return P, Q

# ============================= TRAIN THE FAST MODEL =============================
print("Starting training on filtered dataset...")
start_time = time.time()
P, Q = matrix_factorization_GD_fast(R, k=20, steps=1000, alpha=0.0005, beta=0.02)
print(f"Training completed in {time.time() - start_time:.2f} seconds!\n")

# ============================= RECOMMEND FUNCTION (unchanged) =============================
def recommend_mf_gradient_descent(user_idx, n=5):
    pred = P[user_idx] @ Q.T
    rated = np.where(R[user_idx] > 0)[0]
    unrated = [j for j in range(n_books) if j not in rated]
    top_local_idx = np.argsort(pred[unrated])[-n:][::-1]

    recs = []
    for local_j in top_local_idx:
        global_j = unrated[local_j]
        isbn = id_to_book[global_j]  # fixed: use idx_to_book
        title = book[book['ISBN'] == isbn]['Book-Title'].iloc[0]
        recs.append((title, pred[unrated[local_j]]))
    return recs

# ============================= DEMO RECOMMENDATIONS =============================
print("Your Lightning-Fast Gradient Descent MF Recommender (User 0):")
for i, (title, score) in enumerate(recommend_mf_gradient_descent(0, 6), 1):
    print(f"   {i}. {title[:65]:65} → {score:.2f}")

import heapq
from collections import defaultdict
import random

# =============================================================================
# 5. GALE-SHAPLEY → Match Users to Books (Stable Matching!)
# =============================================================================
print("5. GALE-SHAPLEY → Stable User-Book Matching")

# Let's match 10 users to 10 books stably
# These are actual LOCAL INDICES for the P and Q matrices
sample_users_P_idx = random.sample(range(n_users), 10)
sample_books_Q_idx = random.sample(range(n_books), 10)

# Create a mapping from actual Q_idx to Gale-Shapley's book-side local index (0-9)
q_idx_to_gs_book_idx = {q_idx: gs_idx for gs_idx, q_idx in enumerate(sample_books_Q_idx)}
gs_book_idx_to_q_idx = {gs_idx: q_idx for gs_idx, q_idx in enumerate(sample_books_Q_idx)}

# Gale-Shapley preference dictionaries should use 0-indexed keys (0..9) for men and women
# Keys: Gale-Shapley's man local index (0-9), Values: list of Gale-Shapley's woman local index (0-9)
gs_men_prefs = {gs_idx: [] for gs_idx in range(len(sample_users_P_idx))}
gs_women_prefs = {gs_idx: [] for gs_idx in range(len(sample_books_Q_idx))}

# Populate men's preferences (users are 'men')
for gs_u_idx, actual_u_P_idx in enumerate(sample_users_P_idx):
    scores = []
    for actual_b_Q_idx in sample_books_Q_idx:
        scores.append(P[actual_u_P_idx] @ Q[actual_b_Q_idx].T)

    # Ranked list of actual_b_Q_idx based on scores
    ranked_actual_b_Q_indices = [actual_b_Q_idx for _, actual_b_Q_idx in sorted(zip(scores, sample_books_Q_idx), key=lambda pair: pair[0], reverse=True)]

    # Convert actual_b_Q_idx to gs_book_idx for storing in gs_men_prefs
    gs_men_prefs[gs_u_idx] = [q_idx_to_gs_book_idx[q_idx] for q_idx in ranked_actual_b_Q_indices]


# Populate women's preferences (books are 'women')
for gs_b_idx, actual_b_Q_idx in enumerate(sample_books_Q_idx):
    scores = []
    for actual_u_P_idx in sample_users_P_idx:
        scores.append(P[actual_u_P_idx] @ Q[actual_b_Q_idx].T)

    # Ranked list of actual_u_P_idx based on scores
    ranked_actual_u_P_indices = [actual_u_P_idx for _, actual_u_P_idx in sorted(zip(scores, sample_users_P_idx), key=lambda pair: pair[0], reverse=True)]

    # Map actual_u_P_idx back to their gs_u_idx
    actual_u_P_idx_to_gs_u_idx = {p_idx: gs_idx for gs_idx, p_idx in enumerate(sample_users_P_idx)}

    gs_women_prefs[gs_b_idx] = [actual_u_P_idx_to_gs_u_idx[p_idx] for p_idx in ranked_actual_u_P_indices]

# Gale-Shapley from your class
def gale_shapley_stable_matching(men_prefs, women_prefs):
    n = len(men_prefs)
    women_partner = [-1] * n
    men_free = [True] * n
    proposals = [0] * n
    # women_prefs is now a dict {0: [men_prefs_list], ...} where keys are 0-indexed
    # So directly access women_prefs[w] not list(women_prefs.values())[w]

    while any(men_free):
        m = next(i for i in range(n) if men_free[i])

        # Check if man m has proposed to all women
        if proposals[m] >= len(men_prefs[m]):
            men_free[m] = False
            continue

        w = men_prefs[m][proposals[m]]
        proposals[m] += 1

        if women_partner[w] == -1:
            women_partner[w] = m
            men_free[m] = False
        else:
            current = women_partner[w]
            # Check preferences using the direct dict access
            # women_prefs[w] gives the preference list for woman w
            if women_prefs[w].index(m) < women_prefs[w].index(current):
                women_partner[w] = m
                men_free[m] = False
                men_free[current] = True
    return women_partner

matching = gale_shapley_stable_matching(gs_men_prefs, gs_women_prefs)
print("Stable User-Book Matching (Gale-Shapley):")

# 'matching' is a list where matching[gs_book_idx] = gs_user_idx
for gs_b_idx, gs_u_idx in enumerate(matching):
    if gs_u_idx != -1: # If a book was matched
        # Convert Gale-Shapley local indices back to global User-ID and ISBN
        actual_u_P_idx = sample_users_P_idx[gs_u_idx]
        user_id_global = all_users[actual_u_P_idx]

        actual_b_Q_idx = gs_book_idx_to_q_idx[gs_b_idx]
        book_isbn_global = id_to_book[actual_b_Q_idx]

        title = book[book['ISBN']==book_isbn_global]['Book-Title'].iloc[0]
        print(f"   User {user_id_global} ↔ {title[:50]}")
print()

import matplotlib.pyplot as plt
import networkx as nx
from matplotlib.patches import FancyBboxPatch
import seaborn as sns

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("GENERATING DIAGRAMS FOR YOUR RECOMMENDER SYSTEM")
print("="*70)

# =============================================================================
# DIAGRAM 1: Gale-Shapley Stable Matching (Bipartite Graph)
# =============================================================================
print("1. Gale-Shapley Stable Matching Visualization")

G_bipartite = nx.Graph()
# Corrected: Use sample_users_P_idx from previous cell
users_nodes = [f"U{all_users[user_p_idx]}" for user_p_idx in sample_users_P_idx]
G_bipartite.add_nodes_from(users_nodes, bipartite=0)

books_nodes = []
# Corrected: Use sample_books_Q_idx from previous cell and id_to_book
for book_q_idx in sample_books_Q_idx:
    isbn = id_to_book[book_q_idx]
    title_short = book[book['ISBN']==isbn]['Book-Title'].iloc[0][:20]
    books_nodes.append(f"B: {title_short}")

G_bipartite.add_nodes_from(books_nodes, bipartite=1)

# Add stable matching edges (from your Gale-Shapley result)
# 'matching' is a list where matching[gs_b_idx] = gs_u_idx
for gs_b_idx, gs_u_idx in enumerate(matching):
    if gs_u_idx != -1: # Only add if a match was found
        user_node = users_nodes[gs_u_idx]
        book_node = books_nodes[gs_b_idx]
        G_bipartite.add_edge(user_node, book_node, color='crimson', width=3)

plt.figure(figsize=(12, 8))
pos = {}
pos.update((node, (1, i)) for i, node in enumerate(users_nodes))
pos.update((node, (3, i)) for i, node in enumerate(books_nodes))

# Draw nodes and labels
nx.draw_networkx_nodes(G_bipartite, pos,
                       node_color=['lightblue']*len(users_nodes) + ['lightcoral']*len(books_nodes),
                       node_size=3000)
nx.draw_networkx_labels(G_bipartite, pos, font_size=10, font_weight='bold')

# Draw only the edges that were added (the matched ones) with the specified color
nx.draw_networkx_edges(G_bipartite, pos, edgelist=G_bipartite.edges(), edge_color='crimson', width=3)

plt.title("Gale-Shapley Stable Matching\n(Users ↔ Books)", fontsize=16, fontweight='bold', pad=20)
plt.text(2, len(users_nodes)/2 + 1, "Stable Pairs\n(No blocking pairs)", ha='center', fontsize=12,
         bbox=dict(boxstyle="round,pad=0.8", facecolor="lightgreen", alpha=0.7))
plt.axis('off')
plt.tight_layout()
plt.show()
print()

# =============================================================================
# DIAGRAM 2: User Similarity Graph + Dijkstra Path
# =============================================================================
print("2. User Similarity Graph + Dijkstra Influence Path")

# Create a small subgraph for visualization
G_sim = nx.Graph()
sample_user_indices = [0, 5, 12, 25, 50]  # from your earlier Dijkstra path
for i in sample_user_indices:
    G_sim.add_node(f"U{i}\nID:{all_users[i]}", size=3000)

# Add edges based on common books
for i_idx, i in enumerate(sample_user_indices):
    for j_idx, j in enumerate(sample_user_indices):
        if i < j:
            common = np.sum((R[i] > 0) & (R[j] > 0))
            if common >= 2:
                G_sim.add_edge(f"U{i}\nID:{all_users[i]}", f"U{j}\nID:{all_users[j]}",
                             weight=common, label=common)

plt.figure(figsize=(10, 8))
pos_sim = nx.spring_layout(G_sim, seed=42, k=0.9)

# Highlight the Dijkstra path from earlier
path_nodes = [f"U{p}\nID:{all_users[p]}" for p in path if p in sample_user_indices]
nx.draw(G_sim, pos_sim, with_labels=True, node_color='lightgray', node_size=4000,
        font_size=9, font_weight='bold', edge_color='gray', width=1)

if len(path_nodes) > 1:
    path_edges = [(path_nodes[i], path_nodes[i+1]) for i in range(len(path_nodes)-1)]
    nx.draw_networkx_nodes(G_sim, pos_sim, nodelist=path_nodes, node_color='gold', node_size=5000, edgecolors='red')
    nx.draw_networkx_edges(G_sim, pos_sim, edgelist=path_edges, edge_color='red', width=4)

plt.title("User Similarity Graph\n(Dijkstra Path Highlighted in Red)", fontsize=16, fontweight='bold')
plt.axis('off')
plt.tight_layout()
plt.show()
print()

# =============================================================================
# DIAGRAM 3: Matrix Factorization Architecture (P × Qᵀ)
# =============================================================================
print("3. From-Scratch Matrix Factorization Architecture")

fig, ax = plt.subplots(figsize=(14, 8))
ax.set_xlim(0, 10)
ax.set_ylim(0, 8)
ax.axis('off')

# Rating matrix R
ax.add_patch(FancyBboxPatch((1, 6), 2, 1.5, boxstyle="round,pad=0.5", facecolor="lightblue", edgecolor="black", linewidth=2))
ax.text(2, 6.75, "Rating Matrix R\n(Users × Books)", ha='center', va='center', fontsize=14, fontweight='bold')

# Latent factors
ax.add_patch(FancyBboxPatch((0.5, 3), 1.5, 2, boxstyle="round,pad=0.5", facecolor="lightgreen", edgecolor="black", linewidth=2))
ax.text(1.25, 4, "User Latent\nMatrix P\n(Users × k)", ha='center', va='center', fontsize=12)

ax.add_patch(FancyBboxPatch((7, 3), 1.5, 2, boxstyle="round,pad=0.5", facecolor="lightcoral", edgecolor="black", linewidth=2))
ax.text(7.75, 4, "Book Latent\nMatrix Qᵀ\n(k × Books)", ha='center', va='center', fontsize=12)

# Multiplication
ax.annotate("", xy=(5.5, 4), xytext=(2.5, 4), arrowprops=dict(arrowstyle="->", lw=3, color="purple"))
ax.annotate("", xy=(5.5, 4), xytext=(7.25, 4), arrowprops=dict(arrowstyle="->", lw=3, color="purple"))
ax.text(5.5, 4.5, "×", fontsize=40, ha='center', fontweight='bold', color="purple")

# Predicted ratings
ax.add_patch(FancyBboxPatch((4, 0.5), 2, 1.5, boxstyle="round,pad=0.5", facecolor="gold", edgecolor="black", linewidth=2))
ax.text(5, 1.25, "Predicted Ratings\nP × Qᵀ ≈ R", ha='center', va='center', fontsize=14, fontweight='bold')

# Gradient Descent loop
ax.text(5, 6.5, "↓ Trained using Gradient Descent ↓\n(from your Optimization class)",
        ha='center', fontsize=13, bbox=dict(boxstyle="round,pad=0.8", facecolor="yellow", alpha=0.6))

plt.title("Matrix Factorization via Gradient Descent (Built from Scratch)", fontsize=18, fontweight='bold', pad=30)
plt.tight_layout()
plt.show()

import time
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict
import random
import seaborn as sns

plt.style.use('seaborn-v0_8')
sns.set_palette("deep")
print("CONDUCTING DETAILED NUMERICAL & SCALABILITY ANALYSIS")
print("="*80)

# =============================================================================
# 1. Dataset Sizes for Analysis
# =============================================================================
sizes = [100, 500, 1000, 2000, 5000, 10000]  # Number of ratings
results = []

print(f"Testing on {len(sizes)} problem instances: {sizes} ratings each\n")

for size in sizes:
    print(f"Running analysis on {size:,} ratings...")
    start_total = time.time()

    # Sample subset
    sample_ratings = ratings_final.sample(n=min(size, len(ratings_final)), random_state=42)
    users = sample_ratings['User-ID'].unique()
    books = sample_ratings['ISBN'].unique()

    user_to_idx = {u: i for i, u in enumerate(users)}
    book_to_idx = {b: i for i, b in enumerate(books)}
    n_u, n_b = len(users), len(books)

    R_small = np.zeros((n_u, n_b))
    for row in sample_ratings.itertuples():
        R_small[user_to_idx[row._1], book_to_idx[row.ISBN]] = row._3

    record = {
        'Size (ratings)': size,
        'Users': n_u,
        'Books': n_b,
        'Sparsity (%)': 100 * (1 - np.count_nonzero(R_small) / R_small.size)
    }

# =============================================================================
    # 1. Greedy Popularity (O(n log n))
    # =============================================================================
    start = time.time()
    book_stats = sample_ratings.groupby('ISBN')['Book-Rating'].agg(['count', 'mean'])
    book_stats['score'] = book_stats['mean'] * np.log1p(book_stats['count'])
    top10 = book_stats.sort_values('score', ascending=False).head(10)
    record['Greedy_Time'] = time.time() - start

    # =============================================================================
    # 2. Gradient Descent MF (k=10, 500 steps)
    # =============================================================================
    def mf_gd_small(R, k=10, steps=500, alpha=0.002, beta=0.02):
        P = np.random.normal(scale=1./k, size=(R.shape[0], k))
        Q = np.random.normal(scale=1./k, size=(R.shape[1], k))
        for step in range(steps):
            for i in range(R.shape[0]):
                for j in range(R.shape[1]):
                    if R[i,j] > 0:
                        eij = R[i,j] - np.dot(P[i,:], Q[j,:])
                        P[i,:] += alpha * (2 * eij * Q[j,:] - beta * P[i,:])
                        Q[j,:] += alpha * (2 * eij * P[i,:] - beta * Q[j,:])
        return P, Q

    start = time.time()
    try:
        P_small, Q_small = mf_gd_small(R_small, k=10, steps=500)
        pred = P_small @ Q_small.T
        rmse = np.sqrt(np.mean((pred[R_small > 0] - R_small[R_small > 0])**2))
        record['MF_GD_Time'] = time.time() - start
        record['MF_GD_RMSE'] = rmse
    except:
        record['MF_GD_Time'] = np.nan
        record['MF_GD_RMSE'] = np.nan

    # =============================================================================
    # 3. Dijkstra on User Similarity Graph
    # =============================================================================
    start = time.time()
    try:
        common = R_small @ R_small.T
        G_dijk = defaultdict(list)
        for i in range(n_u):
            for j in range(i+1, min(i+50, n_u)):
                if common[i,j] >= 2:
                    G_dijk[i].append((j, 1/(1+common[i,j])))
                    G_dijk[j].append((i, 1/(1+common[i,j])))

        # Run Dijkstra from user 0 to a random far user
        if len(G_dijk[0]) > 0:
            dist = {i: float('inf') for i in range(n_u)}
            dist[0] = 0
            pq = [(0, 0)]
            visited = 0
            while pq and visited < 100:
                d, u = heapq.heappop(pq)
                if d > dist[u]: continue
                for v, w in G_dijk[u]:
                    if dist[u] + w < dist[v]:
                        dist[v] = dist[u] + w
                        heapq.heappush(pq, (dist[v], v))
                visited += 1
        record['Dijkstra_Time'] = time.time() - start
    except:
        record['Dijkstra_Time'] = np.nan

    # =============================================================================
    # 4. Gale-Shapley (n=20 users/books)
    # =============================================================================
    if n_u >= 20 and n_b >= 20:
        start = time.time()
        sample_u = list(range(20))
        sample_b = list(range(20))
        # Build preferences
        user_prefs = {}
        for u in sample_u:
            scores = [P_small[u] @ Q_small[b].T if u < P_small.shape[0] and b < Q_small.shape[1] else 0 for b in sample_b]
            user_prefs[u] = [sample_b[i] for i in np.argsort(scores)[::-1]]

        book_prefs = {}
        for b in sample_b:
            scores = [P_small[u] @ Q_small[b].T if u < P_small.shape[0] else 0 for u in sample_u]
            book_prefs[b] = [sample_u[i] for i in np.argsort(scores)[::-1]]

        # Run Gale-Shapley
        partner = [-1] * 20
        free = [True] * 20
        prop = [0] * 20
        for _ in range(20):
            m = next((i for i in range(20) if free[i]), None)
            if not m: break
            w = user_prefs[m][prop[m]]
            prop[m] += 1
            if partner[w] == -1:
                partner[w] = m
                free[m] = False
            elif book_prefs[w].index(m) < book_prefs[w].index(partner[w]):
                free[partner[w]] = True
                partner[w] = m
                free[m] = False
        record['GaleShapley_Time'] = time.time() - start
    else:
        record['GaleShapley_Time'] = np.nan

    record['Total_Time'] = time.time() - start_total
    results.append(record)
    print(f"Completed {size:,} ratings | Total time: {record['Total_Time']:.2f}s\n")

# =============================================================================
# RESULTS TABLE & ANALYSIS
# =============================================================================
df_results = pd.DataFrame(results)
print("FINAL NUMERICAL ANALYSIS RESULTS")
print("="*80)
print(df_results.round(4))

# =============================================================================
# PLOTS: Scalability & Performance
# =============================================================================
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Runtime vs Problem Size
axes[0,0].plot(df_results['Size (ratings)'], df_results['Greedy_Time'], 'o-', label='Greedy Popularity', linewidth=3)
axes[0,0].plot(df_results['Size (ratings)'], df_results['MF_GD_Time'], 's-', label='Gradient Descent MF', linewidth=3)
axes[0,0].plot(df_results['Size (ratings)'], df_results['Dijkstra_Time'], '^-', label='Dijkstra Path', linewidth=3)
axes[0,0].plot(df_results['Size (ratings)'], df_results['GaleShapley_Time'], 'd-', label='Gale-Shapley', linewidth=3)
axes[0,0].set_xlabel('Number of Ratings')
axes[0,0].set_ylabel('Time (seconds)')
axes[0,0].set_title('Algorithm Runtime vs Problem Size')
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# 2. MF Accuracy (RMSE)
axes[0,1].plot(df_results['Size (ratings)'], df_results['MF_GD_RMSE'], 'o-', color='purple', linewidth=3, markersize=8)
axes[0,1].set_xlabel('Number of Ratings')
axes[0,1].set_ylabel('RMSE')
axes[0,1].set_title('Matrix Factorization Accuracy (Lower = Better)')
axes[0,1].grid(True, alpha=0.3)

# 3. Sparsity
axes[1,0].plot(df_results['Size (ratings)'], df_results['Sparsity (%)'], 's-', color='red', linewidth=3)
axes[1,0].set_xlabel('Number of Ratings')
axes[1,0].set_ylabel('Sparsity (%)')
axes[1,0].set_title('Data Sparsity Increases with Scale')
axes[1,0].grid(True, alpha=0.3)

# 4. Users & Books
ax2 = axes[1,1]
ax2.plot(df_results['Size (ratings)'], df_results['Users'], 'o-', label='Users', linewidth=3)
ax2.plot(df_results['Size (ratings)'], df_results['Books'], 's-', label='Books', linewidth=3)
ax2.set_xlabel('Number of Ratings')
ax2.set_ylabel('Count')
ax2.set_title('Growth of Users and Books')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.suptitle('COMPREHENSIVE SCALABILITY & PERFORMANCE ANALYSIS\nBook Recommendation Using Pure Class Algorithms',
             fontsize=16, fontweight='bold', y=0.98)
plt.tight_layout()
plt.show()

